# Technology Radar

## 1. Introduction

This document is to write down some notes about various technologies for reference.

## 2. Programming Language and Framework

### 2.1 Bash

### 2.2 Python

#### 2.2.1 Language Feature

Python programs can be decomposed into modules, statements, expressions, and objects, as follows:

1. Programs are composed of modules.
2. Modules contain statements.
3. Statements contain expressions.
4. Expressions create and process objects.

Essential concepts:

* variables and objects, variables refer to objects; variables are created when your code first assign it a value.
* variable scope in different contexts
* types live with objects, not variables
* objects are garbage-collected
* mutable types and immutable types


#### 2.2.2 Framework and Library

### Standard Library

##### twisted

The twisted is an **event-driven networking engine** written in Python.

GitHub <https://github.com/twisted/twisted.git>

Essential concepts:

* Reactor  
* Transports  
* Protocols

##### requests

The requests is an Apache2 Licensed **HTTP library**, written in Python, for human beings.

GitHub <https://github.com/kennethreitz/requests.git>

##### stomper

This is a transport neutral client implementation of the **STOMP** protocol. For example it can be used to connect to [ActiveMQ](http://activemq.apache.org/).

##### kazoo

The kazoo implements a higher level API to [Apache Zookeeper](http://zookeeper.apache.org/) for Python clients.

##### mysql-connector-python

This is a **MySQL** driver written in Python.

#### 2.2.3 Python Virtual Machine

#### 2.2.4 Tools

#### 2.2.5 Best Practice

#### 2.2.6 Reference

### 2.3 C++

#### 2.3.1 Language Feature

##### Class Loader

#### 2.3.2 Framework and Library

##### STL

##### Boost

##### Libevent

#### 2.3.3 Tools

#### 2.3.4 Best Practice

#### 2.3.5 Reference


### 2.4 Java Platform

#### 2.4.1 Language Feature

##### Class Loader

#### 2.4.2 Framework and Library

##### Spring Frameworks

###### Spring Security

Core components:

* SecurityContextHolder, to provide access to the SecurityContext.* SecurityContext, to hold the Authentication and possibly request-specific security information.* Authentication, to represent the principal in a Spring Security-specific manner.* GrantedAuthority, to reflect the application-wide permissions granted to a principal.* UserDetails, to provide the necessary information to build an Authentication object from your application’s DAOs or other source of security data.* UserDetailsService, to create a UserDetails when passed in a String-based username (or certificate ID or the like).

##### Google Guava

##### Netty

#### 2.4.3 Java Virtual Machine

#### 2.4.4 Container

##### Tomcat

##### Jetty

#### 2.4.5 Tools

#### 2.4.6 Best Practice

#### 2.4.7 Reference

* Java native interface with C/C++, SWIG <http://swig.org>

### 2.5 Go


## 3. Platform and Middleware

### 3.1 Linux

## 4. Data Store and Data Cache

### 4.1 MySQL

### 4.2 Memcached

### 4.3 Redis

### 4.4 HBASE

## 5. Distributed System

### 5.1 ZooKeeper

#### 5.1.1 Operating Zookeeper
#### 5.1.2 Machine and Network Configuration

#### 5.1.3 Reconfiguration

#### 5.1.4 Zookeeper Internals

Read requests are served locally; write requests are forwarded to the leader. The perpose of leader is to order updates.

A transaction in zookpeer:  
* is generated by leader  
* carries the lastest data and version number  
* is applied atomically  
* has no rollback mechanism like with traditional relational databases  
* is idempotent and can be applied mutitple times  
* has an identifier called ZooKeeper transaction ID (zxid) which is a long (64-bit) integer splitting into two parts: the *epoch* and the *counter*.

##### Leader Election

##### Broadcasting State Updates

## 6. Big Data
### 6.1 Hadoop

### 6.2 Storm

### 6.3 Spark

## 7. Message Middleware

### 7.1 ActiveMQ
### 7.2 Kafka
### 7.3 RabbitMQ

## 8. Web

### 8.1 Nginx

### 8.2 Apache Httpd

## 9. Others

### 9.1 Solr

Seeing the Solr is a document-based NoSQL datastore. The following dimensions should be explored:

1. data model, related topic: data model design or schema design in Solr's term  
2. storage model  
3. index, inverted index in Solr    
4. distribution model  
5. consistency mode & CAP  
6. read/write performance  
7. failure handling & load balance  
8. scalability  
9. compression  
10. atomic read-modify-write 

#### Indexing
The basic unit of information in Solr is a **document** which is composed of **fields**. For example, a document about a person might contains fields like name, biography, favorite color, and so on. A field has properties, like type, should it be indexed, should it be stored, and so on.  **Field type** tell Solr how to interpret the field and how it can be queried.

![Lucene Indexing Components](https://github.com/rkq/docs/blob/master/pics/lucene-indexing-components.png)

The **schema.xml** defines the fields and field types for your documents.

Schema design:

* What is a document in your index?* How is each document uniquely identified?* What fields in your documents can be searched by users?* Which fields should be displayed to users in the search results?

#### Text Analysis

A primary goal of text analysis is to allow your users to search using natural language without having to worry about all possible forms of their search terms.

In Solr, each **analyzer** breaks the text analysis process into two phases: **tokenization** (parsing) and **token filtering**. Technically, there is also a third phase that enables pre-processing before tokenization, in which you can apply **character filters**.

In practice, it’s common to define two separate analyzers: one for indexing and another for analyzing the text entered by users when searching. 

In the tokenization phase, text is split into a stream of tokens using some form of parsing. All tokenizers produce a stream of tokens that can be processed by zero or more filters to perform some sort of transformation of the token.

A token filter performs one of three actions on a token:

* Transformation - Changing the token to a different form such as lowercasing all letters or stemming
* Token injection - Adding a token to the stream, as is done with the synonym filter
* Token removal - Removing tokens, as is done by the stop word filter

Filters can be chained together to apply a series of transformations on each token. The order of the filters is important as you wouldn’t want to have a filter that depended on the case of your tokens listed after a filter that lowercases all tokens.

#### Query

In Solr, queries are processed by a component called a **searcher**. There is only one "active" searcher in Solr (each core, more precisely) at any given time. All query components for all search request handlers execute queries against the active searcher.

The active searcher has a *read-only* view of a snapshot of the underlying Lucene index. It follows that if you add a new document to Solr, then it is not visible in search results from the current searcher. This raises the question: How do new doc- uments become visible in search results? The answer is to close the current searcher and open a new one that has a read-only view of the updated index. This is what it means to commit documents to Solr.

Because precomputed data, such as a cached query result set, must be invalidated and recomputed, it stands to reason that opening a new searcher on your index is potentially an expensive operation. The good news is that Solr has a number of tools to help alleviate this situation. First and foremost, Solr supports the concept of warming a new searcher in the background and keeping the current searcher active until the new one is fully warmed.

##### Cache Management

Go Solr adminstrtion page, select a core and click **Plugins/Stats**,and **CACHE**.

There are four main concerns when working with Solr caches:

* Cache sizing and eviction policy
* Hit ratio and evictions
* Cached-object invalidation
* Autowarming new caches

Solr keeps all cached objects in memory and does not overflow to disk, as is possible with some caching frameworks. Consequently, Solr requires you to set an upper limit on the number of objects in each cache. Solr will evict objects when the cache reaches the upper limit, using either a Least Recently Used or Least Frequently Used eviction policy.

A common misconception with cache sizing is that you should make your cache sizes quite large if you have the memory available. The problem with this approach is that once a cache becomes invalidated after a commit, there can be many objects that need to be garbage collected by the JVM. Remember, closing a searcher invalidates all cached values. Without proper tuning of garbage collection, this can lead to long pauses in your server, caused by full garbage collection.

Hit ratio is the proportion of cache read requests that result in finding a cached value. The hit ratio indicates how much benefit your application is getting from its cache. 

The eviction count shows how many objects have been evicted from the cache based on the eviction policy. It follows that having a large num- ber of evictions is an indication that the maximum size of your cache may be too small for your application. Eviction count and hit ratio are interrelated, as a high eviction count will lead to a suboptimal hit ratio.

In Solr, all objects in a cache are linked to a specific searcher instance and are immediately invalidated when a searcher is closed. Recall that a searcher is a read-only view of a snapshot of your index; consequently, all cached objects remain valid until the searcher is closed.

Solr creates a new searcher after a commit, but it doesn’t close the old searcher until the new searcher is fully warmed. Some of the keys in the soon-to-be-closed searcher’s cache can be used to populate the new searcher’s cache, a process known as *autowarming* in Solr. Note that autowarming a cache is different than using a warming query to populate a cache. The key point for now is that you can con- figure Solr’s caches to refresh a subset of cached objects when opening a new searcher, but as with any optimization technique, you need to be careful to not overdo it.

###### Filter Cache

In Solr, a **filter** restricts search results to documents that meet the filter criteria, but it does not affect scoring.

In fact, using filters to optimize queries is one of the most powerful features in Solr, mainly because filters are reusable across queries.

Each object in the cache has a key. For the filter cache, the key is the filter query, such as manu:Belkin. To warm the new cache, a subset of keys is pulled from the old cache and executed against the new searcher, which recomputes the filter. Autowarming the filter cache requires Solr to re-execute the filter query with the new searcher. Consequently, autowarming the filter cache can be a source of performance and resource utilization problems in Solr.

We recommend that you enable autowarming for the filter cache, but set the autowarmCount attribute to a small number to start. In addition, we think the LFU eviction policy is more appropriate for the filter cache because it allows you to keep the filter cache small and give priority to the most popular filters in your application.

In terms of memory usage per cached filter, Solr has different filter representations based on the size of the matching document set. As an upper limit, you can figure that any filter that matches many documents in your index will require MaxDoc (the number of documents in the index) bits of memory.

###### Query Result Cache

The query result cache holds result sets for a query.

Behind the scenes, the query result cache holds a query as the key and a list of internal Lucene document IDs as the value. Internal Lucene document IDs can change from one searcher to the next, so the cached values must be recomputed when warming the query result cache.

To warm the query result cache, Solr needs to re-execute queries, which can be expensive. The same advice we gave about keeping the autowarmCount attribute small for the filter cache applies to the query result cache. That said, we do recommend setting the autowarmCount attribute for this cache to something other than the default zero, so that you get some benefit from autowarming recent queries.

###### Document Cache

The query result cache holds a list of internal document IDs that match a query, so even if the query results are cached, Solr still needs to load the documents from disk to produce search results. The document cache is used to store documents loaded from disk in memory keyed by their internal document IDs. It follows that the query result cache uses the document cache to find cached versions of documents in the cached result set.

###### Field Cache


###### Field Value Cache

The field value cache provides fast access to stored field values by internal document ID. It is used during sorting and when building documents for the response.

#### SolrCloud

##### Core Concepts

A Solr **core** is a uniquely named, managed, and configured index running in a Solr server; a Solr server can host one or more cores. A core is typically used to separate documents that have different schemas.

SolrCloud introduces the concept of a **collection**, which extends the concept of a uniquely named, managed, and configured index to one that is split into shards and distributed across multiple servers. The reason SolrCloud needs a new term is because each shard of a distributed index is hosted in a Solr core.

When you bootstrap a new collection in a SolrCloud cluster, you need to upload the configuration files such as solrconfig.xml and schema.xml into ZooKeeper. When Solr launches in cloud mode, it will pull the configuration files from ZooKeeper automatically.

#### Customization

#### Miscellaneous

The solr use a global java system property **solr.solr.home** to identify the root direcotry from which to look for configuration files.

The **solrconfig.xml** defines the main settings for a specific Solr **core**, thus, each core has a solrconfig.xml file.

The file **core.properties** is used to autodsicover cores. Once a core is discovered, Solr locates the solrconfig.xml file under directory which contains core.properties file. Solr uses the solrconfig.xml file to initialize the core.

#### Reference

* <http://lucidworks.com>

